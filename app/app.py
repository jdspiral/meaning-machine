import os
from pathlib import Path

os.environ["TRANSFORMERS_OFFLINE"]   = "1"
os.environ["HF_DATASETS_OFFLINE"]    = "1"
os.environ["HF_METRICS_OFFLINE"]     = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"
os.environ["THINC_BACKEND"]          = "numpy"

import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
from sklearn.decomposition import PCA
import plotly.express as px
import torch
from transformers import AutoTokenizer, AutoModel
import spacy
from spacy import displacy
import gc

st.set_page_config(page_title="Tokenizer", layout="centered")

@st.cache_resource(show_spinner=False)
def load_spacy():
    return spacy.load("en_core_web_sm")

@st.cache_resource(show_spinner=False)
def load_bert():
    local_dir = Path(__file__).resolve().parent / "models" / "distilbert-base-uncased"
    if not local_dir.is_dir():
        raise FileNotFoundError(f"Model folder not found: {local_dir}")
    if not (local_dir / "config.json").is_file():
        raise FileNotFoundError(f"config.json missing in: {local_dir}")

    tokenizer = AutoTokenizer.from_pretrained(
        str(local_dir),
        local_files_only=True
    )
    tokenizer.model_max_length = 512

    model = AutoModel.from_pretrained(
        str(local_dir),
        local_files_only=True
    )
    model.eval()
    return tokenizer, model

nlp = load_spacy()
tokenizer, model = load_bert()

st.title("Tokenizer")
st.markdown("##### Explore how text gets tokenized, embedded, and linguistically analyzed ‚Äî and what that means (and doesn‚Äôt) for LLMs.")
with st.expander("‚ö†Ô∏è Read This First ‚Äî What This App Shows and Doesn‚Äôt", expanded=False):
    st.markdown("""
**This app is educational.** It visualizes classical NLP techniques ‚Äî tokenization, POS tagging, SVO extraction, dependency trees, and embedding visualizations ‚Äî as a way to help users understand **how structure and meaning can be simulated** in language.

But here's the key thing:

- ‚úÖ **Tokenization and embeddings** reflect actual processes used by language models like BERT and GPT.
- ‚ö†Ô∏è **POS tagging, SVO extraction, and dependency parsing** come from classical NLP (via spaCy), not modern LLM internals. They‚Äôre shown here to make implicit structures more legible, not because they are used by the models directly.
- üß† LLMs **do not explicitly tag parts of speech** or build trees. Their knowledge of syntax emerges implicitly in vector space (e.g., via attention heads).

For how transformers actually encode structure, check out:
- [A Structural Probe](https://arxiv.org/abs/1906.04341)
- [BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/abs/1905.05950)
- [Transformer Circuits (Anthropic)](https://transformer-circuits.pub/2022/toy_model/index.html)
""")

with st.form("input_form"):
    user_input = st.text_area(
        "Enter a sentence:",
        "The young student didn't submit the final report on time.",
        height=120,
    )
    submitted = st.form_submit_button("Submit")


def extract_svo(doc):
    svos = []
    for token in doc:
        if token.dep_ in ("ROOT","ccomp","xcomp") and token.pos_ == "VERB":
            subj = [w for w in token.lefts if w.dep_ in ("nsubj","nsubjpass")]
            obj  = [w for w in token.rights if w.dep_ in ("dobj","attr","oprd")]
            if subj and obj:
                svos.append((subj[0].text, token.text, obj[0].text))
    return svos

if user_input:
    st.markdown("## Tokenization (BERT-style)")
    with st.expander("Why Tokenization Matters", expanded=False):
        st.write(
            "Language models do not process text as humans do. Instead they first "
            "break your input into **tokens**‚Äîthe smallest fragments (words, subwords, or symbols) "
            "they know how to handle. Each token maps to a numeric ID in the model's vocabulary."
        )
    encoded = tokenizer(
        user_input,
        return_tensors="pt",
        truncation=True,
        max_length=tokenizer.model_max_length,
        add_special_tokens=False,
    )
    token_ids = encoded["input_ids"][0].cpu().tolist()
    tokens    = tokenizer.convert_ids_to_tokens(token_ids)
    df_tokens = pd.DataFrame({
        "Index": list(range(len(tokens))),
        "Token": tokens,
        "Token ID": token_ids
    })
    st.code(" | ".join(tokens))
    st.table(df_tokens)

    st.markdown("## Subject‚ÄìVerb‚ÄìObject (SVO) Triples")
    with st.expander("About SVO Extraction", expanded=False):
        st.write(
            "We use spaCy's dependency parse to extract simple (subject, verb, object) "
            "triples‚Äîi.e. who did what to whom‚Äîby looking for the main verb (ROOT/ccomp/xcomp) "
            "and its nominal subject and direct object."
        )
    doc = nlp(user_input)
    svos = extract_svo(doc)
    if svos:
        st.table(pd.DataFrame(svos, columns=["Subject","Verb","Object"]))
    else:
        st.warning("No SVO triples detected. Try: 'The dog chased the cat.'")

    st.markdown("## POS Tagging (spaCy)")
    with st.expander("Why POS & Dependencies?", expanded=False):
        st.write(
            "spaCy assigns each token a **Part-Of-Speech (POS) tag**, a **dependency label** "
            "describing its role (e.g., nsubj, dobj), and a **head** (the token it depends on). "
            "This reveals the grammatical skeleton of your sentence."
        )
    df_pos = pd.DataFrame([
        (t.i, t.text, t.pos_, t.dep_, t.head.text) for t in doc
    ], columns=["Index","Token","POS","Dependency","Head"])
    st.table(df_pos)

    st.markdown("## Embedding Visualization (DistilBERT)")
    with st.expander("How Embeddings Work", expanded=False):
        st.write(
            "BERT maps each token into a 768-dimensional vector capturing its contextual meaning. "
            "We reduce those vectors to 2D using PCA so you can see clusters of similar tokens."
        )
    with torch.no_grad():
        outputs = model(**encoded)
    embeddings = outputs.last_hidden_state[0].cpu().numpy()
    if embeddings.shape[0] < 2:
        st.warning("Not enough tokens for 2D embedding plot.")
    else:
        coords = PCA(n_components=2).fit_transform(embeddings)
        pos_labels = []
        for tok in doc:
            subtoks = tokenizer.tokenize(tok.text)
            pos_labels.extend([tok.pos_] * len(subtoks))
        n = min(len(tokens), coords.shape[0], len(pos_labels))
        df_plot = pd.DataFrame({
            "x": coords[:n,0],
            "y": coords[:n,1],
            "token": tokens[:n],
            "type": pos_labels[:n],
        })
        fig = px.scatter(df_plot, x="x", y="y", text="token", color="type",
                         title="Token Embeddings PCA")
        fig.update_traces(textposition="top center")
        st.plotly_chart(fig, use_container_width=True)

    del outputs, embeddings, coords
    torch.cuda.empty_cache()
    gc.collect()

    st.markdown("## Dependency Structure")
    with st.expander("Reading the Tree", expanded=False):
        st.write(
            "Here we render the full dependency parse tree‚Äîshowing how subjects, objects, "
            "modifiers and clauses connect to form the sentence structure."
        )
    html = displacy.render(doc, style="dep", page=False)
    components.html(html, scrolling=True, height=480)
    st.write('Scroll ‚ÜîÔ∏è to see the full tree.')

    st.markdown("## Dependency Role Glossary")
    with st.expander("Tap to expand", expanded=True):
        st.markdown("""
| **Label**   | **Name**                  | **Description**                                                              |
|-------------|---------------------------|------------------------------------------------------------------------------|
| `nsubj`     | Nominal Subject           | The noun performing the main action (e.g., ‚Äústudent‚Äù).                       |
| `aux`       | Auxiliary Verb            | A helper verb (e.g., ‚Äúdid‚Äù).                                                |
| `neg`       | Negation Modifier         | Negates another word (e.g., ‚Äún't‚Äù).                                          |
| `ROOT`      | Root                      | The sentence‚Äôs main predicate (main verb).                                   |
| `advmod`    | Adverbial Modifier        | Modifies a verb or clause (e.g., ‚Äúon time‚Äù).                                 |
| `prep`      | Prepositional Modifier    | Introduces a prepositional phrase (e.g., ‚Äúon‚Äù).                              |
| `pobj`      | Object of Preposition     | The noun in a prepositional phrase (e.g., ‚Äútime‚Äù).                           |
| `det`       | Determiner                | Introduces or limits a noun (e.g., ‚ÄúThe‚Äù).                                   |
| `attr`      | Attribute                 | Describes subject after a linking verb (e.g., ‚Äúis‚Äù).                         |
| `amod`      | Adjectival Modifier       | An adjective modifying a noun (e.g., ‚Äúyoung‚Äù).                               |
| `compound`  | Compound Modifier         | A noun modifying another noun (e.g., ‚Äúfinal‚Äù).                               |
| `cc`        | Coordinating Conjunction  | Connects equal elements (e.g., ‚Äúand,‚Äù ‚Äúbut‚Äù).                                |
| `conj`      | Conjunct                  | The second element joined by a CC.                                            |
| `ccomp`     | Clausal Complement        | A clause acting as an object (e.g., ‚ÄúI think that he left‚Äù).                 |
| `xcomp`     | Open Clausal Complement   | A clause without its own subject (e.g., ‚Äúto leave‚Äù).                         |
| `mark`      | Marker                    | Introduces a subordinate clause (e.g., ‚Äúthat‚Äù).                              |
"""
        )
        st.caption("Aligned with: ‚ÄúThe young student didn't submit the final report on time.‚Äù")

st.markdown("---")

st.markdown("### üîó Follow & Deploy")
st.markdown("""
- üîó [Read Explainers](https://theperformanceage.com/s/explainers)  
- üíª [GitHub](https://github.com/jdspiral/tokenizer)  
- ‚úñÔ∏è [X](https://twitter.com/joshdhathcock)  
- üì∏ [Instagram](https://instagram.com/joshdhathcock)  
""")
st.caption("Built to think out loud with code by Josh Hathcock @ The Performance Age")
